import os
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import SentenceTransformersTokenTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Set a polite User-Agent
os.environ["USER_AGENT"] = "MyLangchainBot/1.0 (+https://example.com)"

def get_internal_links(base_url, limit=10):
    """Extract internal links from any website (same domain only)."""
    try:
        response = requests.get(base_url, headers={"User-Agent": os.environ["USER_AGENT"]}, timeout=10)
        response.raise_for_status()
    except Exception as e:
        print(f"âŒ Failed to fetch {base_url}: {e}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    domain = urlparse(base_url).netloc
    links = set()

    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        full_url = urljoin(base_url, href)
        parsed = urlparse(full_url)

        # Only keep links within the same domain and HTTP(S)
        if parsed.netloc == domain and parsed.scheme in ["http", "https"]:
            links.add(full_url)

        if len(links) >= limit:
            break

    return list(links)


def crawl_and_embed(base_url, link_limit=10):
    """Crawl a website and create FAISS embeddings."""
    # 1ï¸âƒ£ Get related internal links
    related_links = get_internal_links(base_url, limit=link_limit)
    urls = [base_url] + related_links

    # 2ï¸âƒ£ Load all pages
    all_documents = []
    for url in urls:
        loader = WebBaseLoader(url)
        try:
            docs = loader.load()
            all_documents.extend(docs)
            print(f"âœ… Loaded {url}")
        except Exception as e:
            print(f"âš ï¸ Failed to load {url}: {e}")

    print(f"ğŸ“„ Loaded {len(all_documents)} total documents.")

    if not all_documents:
        print("âŒ No documents loaded. Exiting.")
        return

    # 3ï¸âƒ£ Split into chunks
    text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=512, chunk_overlap=50)
    texts = text_splitter.split_documents(all_documents)
    print(f"âœ‚ï¸ Split into {len(texts)} chunks.")

    # 4ï¸âƒ£ Generate embeddings
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # 5ï¸âƒ£ Build and save FAISS index
    db = FAISS.from_documents(texts, embeddings)

    FAISS_INDEX_PATH = os.path.join("faiss_data", "faiss_index")
    os.makedirs("faiss_data", exist_ok=True)
    db.save_local(FAISS_INDEX_PATH)
    print("ğŸ’¾ Vector store created and saved to faiss_data/faiss_index")


if __name__ == "__main__":
    # Example: crawl any site â€” not just Wikipedia
    base_url = "https://wiki.python.org/moin/BeginnersGuide"  # change this to any site
    crawl_and_embed(base_url, link_limit=10)
